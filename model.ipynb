{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminfarcy/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Reading ...\n",
      "-- Done\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lib.model import train_lgbm_fold_classif, plot_importances, train_lgbm_fold_reg\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from lib.constants import DATA_FOLDER, TMP_FOLDER, SUBMISSION_FOLDER\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lib.dataload import load_data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from lib.utils import make_submission_from_hdf\n",
    "df_train, df_target, df_test = load_data(read=True, reduce_mem=False)\n",
    "df_train['is_train'] = 1\n",
    "df_test['is_train'] = 0\n",
    "train_test = pd.concat([df_train, df_test], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 203)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 211)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminfarcy/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/generic.py:1315: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n",
      "100%|██████████| 14/14 [03:09<00:00, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 1303)\n",
      "(400000, 1303)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "remove_cols = ['target', 'ID_code', 'is_train', 'key_0', 'strat', 'oof_preds_ref', 'oof_preds_ref_error']\n",
    "selected_cols = list(set(train_test.columns) - set(remove_cols))\n",
    "\n",
    "train_test[selected_cols] = scaler.fit_transform(train_test[selected_cols])/3\n",
    "\"\"\"\n",
    "binned_cols = []\n",
    "for col in tqdm(selected_cols):\n",
    "    newcol = 'binned_' + col\n",
    "    binned_cols.append(newcol)\n",
    "    train_test[newcol] = pd.cut(train_test[col], 50, labels=[x for x in range(0, 50)])\n",
    "    tmp = pd.cut(train_test[col], 50, labels=[x for x in range(0, 50)]).value_counts().sort_values().reset_index().reset_index()\n",
    "    tmp = tmp[['level_0', 'index']].rename(columns={'index':'binned_' + col, 'level_0':'rerank_' + col})\n",
    "    train_test = train_test.merge(tmp, on=['binned_' + col])\n",
    "\n",
    "\"\"\"\n",
    "# columnwise features\n",
    "\n",
    "train_test['sum_col'] = train_test[selected_cols].sum(axis=1)\n",
    "train_test['min_col'] = train_test[selected_cols].min(axis=1)\n",
    "train_test['max_col'] = train_test[selected_cols].max(axis=1)\n",
    "train_test['std_col'] = train_test[selected_cols].std(axis=1)\n",
    "train_test['var_col'] = train_test[selected_cols].var(axis=1)\n",
    "train_test['mean_col'] = train_test[selected_cols].mean(axis=1)\n",
    "train_test['median_col'] = train_test[selected_cols].median(axis=1)\n",
    "train_test['spread_col'] = abs(train_test['max_col'] - train_test['min_col'])\n",
    "print(train_test.shape)\n",
    "\n",
    "\"\"\"\n",
    "# modulo stuff\n",
    "modulo_cols = []\n",
    "list_modulos = [0.05, 0.1, 0.2]\n",
    "for col in tqdm(selected_cols):\n",
    "    for modulo in list_modulos:\n",
    "        new_col = col + '_modulo_' + str(modulo)\n",
    "        modulo_cols.append(new_col)\n",
    "        train_test[new_col] = train_test[col] % modulo\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "binned_cols = []\n",
    "for col in selected_cols:\n",
    "    newcol = 'binned_' + col\n",
    "    binned_cols.append(newcol)\n",
    "    train_test[newcol] = pd.qcut(train_test[col], 10, labels=[x for x in range(0, 10)])\n",
    "\"\"\"\n",
    "top_features = [\n",
    "    'var_6', 'var_34', 'var_53', 'var_22',\n",
    "    'var_174', 'var_99', 'var_12', 'var_81',\n",
    "    'var_68', 'var_12', 'var_108', 'var_126',\n",
    "    'var_33', 'var_139']\n",
    "\n",
    "\n",
    "# try a groupby\n",
    "def perform_agg_dict(data, agg_dict_ref, groupcol):\n",
    "    agg_dict = {}\n",
    "    for col in agg_dict_ref.keys():\n",
    "        agg_dict[col] = {}\n",
    "        for aggfunc in agg_dict_ref[col]:\n",
    "            if isinstance(aggfunc, str):\n",
    "                func_name = aggfunc\n",
    "            else:\n",
    "                func_name = aggfunc.__name__\n",
    "            agg_dict[col][col + '_' + \"-\".join(groupcol) + '_' + func_name] = aggfunc\n",
    "\n",
    "    tmp = data.groupby(groupcol).agg(agg_dict)\n",
    "    tmp.columns = tmp.columns.droplevel()\n",
    "    tmp = tmp.reset_index()\n",
    "    return tmp\n",
    "agg_dict_ref_col = {}\n",
    "for col in top_features:\n",
    "    agg_dict_ref_col[col] = ['min', 'max', 'median', 'mean', 'var', 'std']\n",
    "for groupcol in tqdm(top_features):\n",
    "    tmp = perform_agg_dict(train_test, agg_dict_ref_col, groupcol)\n",
    "    tmp = tmp.fillna(0)\n",
    "    train_test = train_test.merge(tmp, on=groupcol)\n",
    "print(train_test.shape)\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Create PolynomialFeatures object with interaction_only set to True\n",
    "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "\n",
    "#top_features = ['var_6', 'var_34', 'var_53', 'var_22', 'var_174']\n",
    "# Transform feature matrix\n",
    "\n",
    "poly_interactions = pd.DataFrame(interaction.fit_transform(train_test[top_features]),\n",
    "                                 index=train_test.index,\n",
    "                                 columns=interaction.get_feature_names()\n",
    "                                )\n",
    "poly_interactions['ID_code'] = train_test['ID_code']\n",
    "\n",
    "train_test = (\n",
    "    train_test.merge(poly_interactions[['ID_code'] + interaction.get_feature_names()[len(top_features):]],\n",
    "                     on=['ID_code'])\n",
    ")\n",
    "print(train_test.shape)\n",
    "for feat in top_features:\n",
    "    for other_feat in top_features:\n",
    "        if feat != other_feat:\n",
    "            intername = feat + '_-_' + other_feat\n",
    "            train_test[intername] = train_test[feat] - train_test[other_feat]\n",
    "            intername = feat + '_+_' + other_feat\n",
    "            train_test[intername] = train_test[feat] + train_test[other_feat]\n",
    "\n",
    "print(train_test.shape)\n",
    "\"\"\"\n",
    "#train_test = train_test.fillna(0)\n",
    "\n",
    "print(train_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in binned_cols:\n",
    "    train_test[col] = train_test[col].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Resplit train/test\n"
     ]
    }
   ],
   "source": [
    "print(\"- Resplit train/test\")\n",
    "train = train_test[train_test['is_train'] == 1]\n",
    "test = train_test[train_test['is_train'] == 0]\n",
    "train.to_hdf('./data_tmp/df_train_fe.hdf', 'df')\n",
    "test.to_hdf('./data_tmp/df_test_fe.hdf', 'df')\n",
    "pd.DataFrame(train['target']).to_hdf('./data_tmp/df_target_fe2.hdf', 'df')\n",
    "#oof_preds_ref = pd.read_hdf('./data_tmp/oof_lgbm_classif_CV_0.89904_TR_0.91199.hdf', key='df')\n",
    "#train = train.merge(oof_preds_ref, on=train.index).rename(columns={0: 'oof_preds_ref'})\n",
    "#train['oof_preds_ref_error'] = abs(train['oof_preds_ref'] - train['target'])\n",
    "#train['oof_preds_ref'] = np.round(train['oof_preds_ref'])\n",
    "#train['strat'] = ((train['target'] == 1) | (train['oof_preds_ref_error'] > 0.6)).astype('int')\n",
    "#train[train['strat'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminfarcy/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== INIT ==\n",
      "== START MODEL TRAIN\n",
      "== REPEAT CV 0\n",
      "==== CV 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.756083\tvalid_1's auc: 0.75034\n",
      "[200]\ttraining's auc: 0.782589\tvalid_1's auc: 0.774733\n",
      "[300]\ttraining's auc: 0.800296\tvalid_1's auc: 0.791501\n",
      "[400]\ttraining's auc: 0.813047\tvalid_1's auc: 0.803303\n",
      "[500]\ttraining's auc: 0.822911\tvalid_1's auc: 0.81273\n",
      "[600]\ttraining's auc: 0.831508\tvalid_1's auc: 0.821033\n",
      "[700]\ttraining's auc: 0.83865\tvalid_1's auc: 0.828084\n",
      "[800]\ttraining's auc: 0.844349\tvalid_1's auc: 0.833544\n",
      "[900]\ttraining's auc: 0.849654\tvalid_1's auc: 0.838478\n",
      "[1000]\ttraining's auc: 0.854006\tvalid_1's auc: 0.842567\n",
      "[1100]\ttraining's auc: 0.858078\tvalid_1's auc: 0.846562\n",
      "[1200]\ttraining's auc: 0.86152\tvalid_1's auc: 0.849939\n",
      "[1300]\ttraining's auc: 0.864636\tvalid_1's auc: 0.852979\n",
      "[1400]\ttraining's auc: 0.867604\tvalid_1's auc: 0.855691\n",
      "[1500]\ttraining's auc: 0.870613\tvalid_1's auc: 0.858326\n",
      "[1600]\ttraining's auc: 0.872866\tvalid_1's auc: 0.860453\n",
      "[1700]\ttraining's auc: 0.875066\tvalid_1's auc: 0.862432\n",
      "[1800]\ttraining's auc: 0.877071\tvalid_1's auc: 0.864149\n",
      "[1900]\ttraining's auc: 0.878786\tvalid_1's auc: 0.865855\n",
      "[2000]\ttraining's auc: 0.88058\tvalid_1's auc: 0.867646\n",
      "[2100]\ttraining's auc: 0.882033\tvalid_1's auc: 0.868964\n",
      "[2200]\ttraining's auc: 0.883661\tvalid_1's auc: 0.870336\n",
      "[2300]\ttraining's auc: 0.885347\tvalid_1's auc: 0.871911\n",
      "[2400]\ttraining's auc: 0.886827\tvalid_1's auc: 0.87334\n",
      "[2500]\ttraining's auc: 0.888076\tvalid_1's auc: 0.874356\n",
      "[2600]\ttraining's auc: 0.889444\tvalid_1's auc: 0.875367\n",
      "[2700]\ttraining's auc: 0.890618\tvalid_1's auc: 0.876399\n",
      "[2800]\ttraining's auc: 0.891728\tvalid_1's auc: 0.877288\n",
      "[2900]\ttraining's auc: 0.892663\tvalid_1's auc: 0.8782\n",
      "[3000]\ttraining's auc: 0.893726\tvalid_1's auc: 0.87908\n",
      "[3100]\ttraining's auc: 0.894665\tvalid_1's auc: 0.879937\n",
      "[3200]\ttraining's auc: 0.89565\tvalid_1's auc: 0.88069\n",
      "[3300]\ttraining's auc: 0.896358\tvalid_1's auc: 0.881288\n",
      "[3400]\ttraining's auc: 0.897442\tvalid_1's auc: 0.882098\n",
      "[3500]\ttraining's auc: 0.898427\tvalid_1's auc: 0.883002\n",
      "[3600]\ttraining's auc: 0.899126\tvalid_1's auc: 0.883397\n",
      "[3700]\ttraining's auc: 0.89966\tvalid_1's auc: 0.883742\n",
      "[3800]\ttraining's auc: 0.900269\tvalid_1's auc: 0.884168\n",
      "[3900]\ttraining's auc: 0.900942\tvalid_1's auc: 0.884725\n",
      "[4000]\ttraining's auc: 0.901517\tvalid_1's auc: 0.885219\n",
      "[4100]\ttraining's auc: 0.902034\tvalid_1's auc: 0.88557\n",
      "[4200]\ttraining's auc: 0.902509\tvalid_1's auc: 0.886043\n",
      "[4300]\ttraining's auc: 0.903068\tvalid_1's auc: 0.886394\n",
      "[4400]\ttraining's auc: 0.903661\tvalid_1's auc: 0.886749\n",
      "[4500]\ttraining's auc: 0.904122\tvalid_1's auc: 0.887019\n",
      "[4600]\ttraining's auc: 0.904576\tvalid_1's auc: 0.887282\n",
      "[4700]\ttraining's auc: 0.905029\tvalid_1's auc: 0.887676\n",
      "[4800]\ttraining's auc: 0.905442\tvalid_1's auc: 0.887966\n",
      "[4900]\ttraining's auc: 0.905843\tvalid_1's auc: 0.888394\n",
      "[5000]\ttraining's auc: 0.906433\tvalid_1's auc: 0.888769\n",
      "[5100]\ttraining's auc: 0.906849\tvalid_1's auc: 0.88908\n",
      "[5200]\ttraining's auc: 0.907217\tvalid_1's auc: 0.88929\n",
      "[5300]\ttraining's auc: 0.907584\tvalid_1's auc: 0.889484\n",
      "[5400]\ttraining's auc: 0.907951\tvalid_1's auc: 0.889827\n",
      "[5500]\ttraining's auc: 0.908208\tvalid_1's auc: 0.890012\n",
      "[5600]\ttraining's auc: 0.908457\tvalid_1's auc: 0.890219\n",
      "[5700]\ttraining's auc: 0.908776\tvalid_1's auc: 0.89035\n",
      "[5800]\ttraining's auc: 0.909107\tvalid_1's auc: 0.89069\n",
      "[5900]\ttraining's auc: 0.909443\tvalid_1's auc: 0.890832\n",
      "[6000]\ttraining's auc: 0.909773\tvalid_1's auc: 0.890968\n",
      "[6100]\ttraining's auc: 0.910069\tvalid_1's auc: 0.89119\n",
      "[6200]\ttraining's auc: 0.910332\tvalid_1's auc: 0.891371\n",
      "[6300]\ttraining's auc: 0.910619\tvalid_1's auc: 0.891424\n",
      "[6400]\ttraining's auc: 0.910898\tvalid_1's auc: 0.891586\n",
      "[6500]\ttraining's auc: 0.911118\tvalid_1's auc: 0.891717\n",
      "[6600]\ttraining's auc: 0.911402\tvalid_1's auc: 0.89171\n",
      "Early stopping, best iteration is:\n",
      "[6502]\ttraining's auc: 0.911124\tvalid_1's auc: 0.891726\n",
      "==== CV 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.755675\tvalid_1's auc: 0.752085\n",
      "[200]\ttraining's auc: 0.781559\tvalid_1's auc: 0.777241\n",
      "[300]\ttraining's auc: 0.799508\tvalid_1's auc: 0.794252\n",
      "[400]\ttraining's auc: 0.812819\tvalid_1's auc: 0.807026\n",
      "[500]\ttraining's auc: 0.822876\tvalid_1's auc: 0.816423\n",
      "[600]\ttraining's auc: 0.830992\tvalid_1's auc: 0.82393\n",
      "[700]\ttraining's auc: 0.838048\tvalid_1's auc: 0.830589\n",
      "[800]\ttraining's auc: 0.844158\tvalid_1's auc: 0.836347\n",
      "[900]\ttraining's auc: 0.849324\tvalid_1's auc: 0.841216\n",
      "[1000]\ttraining's auc: 0.853846\tvalid_1's auc: 0.845454\n",
      "[1100]\ttraining's auc: 0.857755\tvalid_1's auc: 0.848906\n",
      "[1200]\ttraining's auc: 0.861353\tvalid_1's auc: 0.852036\n",
      "[1300]\ttraining's auc: 0.864626\tvalid_1's auc: 0.855056\n",
      "[1400]\ttraining's auc: 0.867392\tvalid_1's auc: 0.857922\n",
      "[1500]\ttraining's auc: 0.869911\tvalid_1's auc: 0.86018\n",
      "[1600]\ttraining's auc: 0.872381\tvalid_1's auc: 0.862273\n",
      "[1700]\ttraining's auc: 0.874776\tvalid_1's auc: 0.864482\n",
      "[1800]\ttraining's auc: 0.876616\tvalid_1's auc: 0.866134\n",
      "[1900]\ttraining's auc: 0.878677\tvalid_1's auc: 0.868171\n",
      "[2000]\ttraining's auc: 0.880521\tvalid_1's auc: 0.86989\n",
      "[2100]\ttraining's auc: 0.882204\tvalid_1's auc: 0.871516\n",
      "[2200]\ttraining's auc: 0.884002\tvalid_1's auc: 0.873262\n",
      "[2300]\ttraining's auc: 0.885184\tvalid_1's auc: 0.874508\n",
      "[2400]\ttraining's auc: 0.88649\tvalid_1's auc: 0.875641\n",
      "[2500]\ttraining's auc: 0.887824\tvalid_1's auc: 0.876788\n",
      "[2600]\ttraining's auc: 0.889021\tvalid_1's auc: 0.877952\n",
      "[2700]\ttraining's auc: 0.890086\tvalid_1's auc: 0.878939\n",
      "[2800]\ttraining's auc: 0.891159\tvalid_1's auc: 0.880043\n",
      "[2900]\ttraining's auc: 0.89245\tvalid_1's auc: 0.881416\n",
      "[3000]\ttraining's auc: 0.893467\tvalid_1's auc: 0.882248\n",
      "[3100]\ttraining's auc: 0.894341\tvalid_1's auc: 0.88309\n",
      "[3200]\ttraining's auc: 0.895343\tvalid_1's auc: 0.883853\n",
      "[3300]\ttraining's auc: 0.89604\tvalid_1's auc: 0.884542\n",
      "[3400]\ttraining's auc: 0.89671\tvalid_1's auc: 0.885023\n",
      "[3500]\ttraining's auc: 0.897638\tvalid_1's auc: 0.885765\n",
      "[3600]\ttraining's auc: 0.898332\tvalid_1's auc: 0.886295\n",
      "[3700]\ttraining's auc: 0.899036\tvalid_1's auc: 0.886909\n",
      "[3800]\ttraining's auc: 0.899669\tvalid_1's auc: 0.887597\n",
      "[3900]\ttraining's auc: 0.900205\tvalid_1's auc: 0.888043\n",
      "[4000]\ttraining's auc: 0.900836\tvalid_1's auc: 0.88852\n",
      "[4100]\ttraining's auc: 0.901655\tvalid_1's auc: 0.889217\n",
      "[4200]\ttraining's auc: 0.902121\tvalid_1's auc: 0.889554\n",
      "[4300]\ttraining's auc: 0.902629\tvalid_1's auc: 0.889835\n",
      "[4400]\ttraining's auc: 0.903141\tvalid_1's auc: 0.890149\n",
      "[4500]\ttraining's auc: 0.903558\tvalid_1's auc: 0.890322\n",
      "[4600]\ttraining's auc: 0.903962\tvalid_1's auc: 0.890604\n",
      "[4700]\ttraining's auc: 0.904382\tvalid_1's auc: 0.890906\n",
      "[4800]\ttraining's auc: 0.90484\tvalid_1's auc: 0.89119\n",
      "[4900]\ttraining's auc: 0.905402\tvalid_1's auc: 0.891629\n",
      "[5000]\ttraining's auc: 0.905905\tvalid_1's auc: 0.892131\n",
      "[5100]\ttraining's auc: 0.906285\tvalid_1's auc: 0.892314\n",
      "[5200]\ttraining's auc: 0.906654\tvalid_1's auc: 0.892375\n",
      "[5300]\ttraining's auc: 0.90705\tvalid_1's auc: 0.892517\n",
      "[5400]\ttraining's auc: 0.907338\tvalid_1's auc: 0.892717\n",
      "[5500]\ttraining's auc: 0.907712\tvalid_1's auc: 0.892781\n",
      "[5600]\ttraining's auc: 0.907973\tvalid_1's auc: 0.892958\n",
      "[5700]\ttraining's auc: 0.908192\tvalid_1's auc: 0.893157\n",
      "[5800]\ttraining's auc: 0.908526\tvalid_1's auc: 0.893443\n",
      "[5900]\ttraining's auc: 0.908803\tvalid_1's auc: 0.893499\n",
      "[6000]\ttraining's auc: 0.909216\tvalid_1's auc: 0.893868\n",
      "[6100]\ttraining's auc: 0.909502\tvalid_1's auc: 0.893929\n",
      "[6200]\ttraining's auc: 0.909827\tvalid_1's auc: 0.893989\n",
      "[6300]\ttraining's auc: 0.910093\tvalid_1's auc: 0.89411\n",
      "[6400]\ttraining's auc: 0.910331\tvalid_1's auc: 0.894156\n",
      "[6500]\ttraining's auc: 0.910585\tvalid_1's auc: 0.894233\n",
      "[6600]\ttraining's auc: 0.910825\tvalid_1's auc: 0.894389\n",
      "[6700]\ttraining's auc: 0.91111\tvalid_1's auc: 0.894623\n",
      "[6800]\ttraining's auc: 0.911445\tvalid_1's auc: 0.894684\n",
      "[6900]\ttraining's auc: 0.911696\tvalid_1's auc: 0.894728\n",
      "[7000]\ttraining's auc: 0.911904\tvalid_1's auc: 0.894807\n",
      "[7100]\ttraining's auc: 0.912134\tvalid_1's auc: 0.894998\n",
      "[7200]\ttraining's auc: 0.912347\tvalid_1's auc: 0.895096\n",
      "[7300]\ttraining's auc: 0.912587\tvalid_1's auc: 0.895148\n",
      "Early stopping, best iteration is:\n",
      "[7259]\ttraining's auc: 0.912494\tvalid_1's auc: 0.895164\n",
      "==== CV 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.75398\tvalid_1's auc: 0.75537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttraining's auc: 0.780745\tvalid_1's auc: 0.780658\n",
      "[300]\ttraining's auc: 0.798987\tvalid_1's auc: 0.797775\n",
      "[400]\ttraining's auc: 0.811616\tvalid_1's auc: 0.809874\n",
      "[500]\ttraining's auc: 0.82197\tvalid_1's auc: 0.819829\n",
      "[600]\ttraining's auc: 0.83023\tvalid_1's auc: 0.827689\n",
      "[700]\ttraining's auc: 0.837101\tvalid_1's auc: 0.834535\n",
      "[800]\ttraining's auc: 0.843077\tvalid_1's auc: 0.840452\n",
      "[900]\ttraining's auc: 0.84838\tvalid_1's auc: 0.845477\n",
      "[1000]\ttraining's auc: 0.852783\tvalid_1's auc: 0.849613\n",
      "[1100]\ttraining's auc: 0.856562\tvalid_1's auc: 0.853475\n",
      "[1200]\ttraining's auc: 0.860054\tvalid_1's auc: 0.856311\n",
      "[1300]\ttraining's auc: 0.863131\tvalid_1's auc: 0.859169\n",
      "[1400]\ttraining's auc: 0.86622\tvalid_1's auc: 0.861749\n",
      "[1500]\ttraining's auc: 0.868698\tvalid_1's auc: 0.864086\n",
      "[1600]\ttraining's auc: 0.871094\tvalid_1's auc: 0.866128\n",
      "[1700]\ttraining's auc: 0.873464\tvalid_1's auc: 0.868282\n",
      "[1800]\ttraining's auc: 0.875541\tvalid_1's auc: 0.870139\n",
      "[1900]\ttraining's auc: 0.877478\tvalid_1's auc: 0.871773\n",
      "[2000]\ttraining's auc: 0.879274\tvalid_1's auc: 0.873501\n",
      "[2100]\ttraining's auc: 0.881486\tvalid_1's auc: 0.875422\n",
      "[2200]\ttraining's auc: 0.882945\tvalid_1's auc: 0.876824\n",
      "[2300]\ttraining's auc: 0.884415\tvalid_1's auc: 0.878065\n",
      "[2400]\ttraining's auc: 0.885988\tvalid_1's auc: 0.879475\n",
      "[2500]\ttraining's auc: 0.887346\tvalid_1's auc: 0.880708\n",
      "[2600]\ttraining's auc: 0.888583\tvalid_1's auc: 0.881873\n",
      "[2700]\ttraining's auc: 0.889906\tvalid_1's auc: 0.883047\n",
      "[2800]\ttraining's auc: 0.890959\tvalid_1's auc: 0.883941\n",
      "[2900]\ttraining's auc: 0.891964\tvalid_1's auc: 0.884898\n",
      "[3000]\ttraining's auc: 0.89299\tvalid_1's auc: 0.885755\n",
      "[3100]\ttraining's auc: 0.893837\tvalid_1's auc: 0.886641\n",
      "[3200]\ttraining's auc: 0.894735\tvalid_1's auc: 0.887317\n",
      "[3300]\ttraining's auc: 0.895457\tvalid_1's auc: 0.887675\n",
      "[3400]\ttraining's auc: 0.896229\tvalid_1's auc: 0.888277\n",
      "[3500]\ttraining's auc: 0.896908\tvalid_1's auc: 0.888819\n",
      "[3600]\ttraining's auc: 0.897533\tvalid_1's auc: 0.889281\n",
      "[3700]\ttraining's auc: 0.898194\tvalid_1's auc: 0.889604\n",
      "[3800]\ttraining's auc: 0.898654\tvalid_1's auc: 0.890002\n",
      "[3900]\ttraining's auc: 0.899369\tvalid_1's auc: 0.890575\n",
      "[4000]\ttraining's auc: 0.899984\tvalid_1's auc: 0.890974\n",
      "[4100]\ttraining's auc: 0.900638\tvalid_1's auc: 0.891491\n",
      "[4200]\ttraining's auc: 0.901154\tvalid_1's auc: 0.891718\n",
      "[4300]\ttraining's auc: 0.901535\tvalid_1's auc: 0.89192\n",
      "[4400]\ttraining's auc: 0.902064\tvalid_1's auc: 0.892259\n",
      "[4500]\ttraining's auc: 0.902474\tvalid_1's auc: 0.89255\n",
      "[4600]\ttraining's auc: 0.902986\tvalid_1's auc: 0.892896\n",
      "[4700]\ttraining's auc: 0.903528\tvalid_1's auc: 0.893324\n",
      "[4800]\ttraining's auc: 0.904127\tvalid_1's auc: 0.893735\n",
      "[4900]\ttraining's auc: 0.904428\tvalid_1's auc: 0.893855\n",
      "[5000]\ttraining's auc: 0.904783\tvalid_1's auc: 0.894197\n",
      "[5100]\ttraining's auc: 0.905169\tvalid_1's auc: 0.894465\n",
      "[5200]\ttraining's auc: 0.905592\tvalid_1's auc: 0.894588\n",
      "[5300]\ttraining's auc: 0.906114\tvalid_1's auc: 0.894936\n",
      "[5400]\ttraining's auc: 0.90642\tvalid_1's auc: 0.895062\n",
      "[5500]\ttraining's auc: 0.906751\tvalid_1's auc: 0.895285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-000e86f10421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#keep_index=keep_that,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrepeat_cv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     n_max_estimators=10000)\n\u001b[0m",
      "\u001b[0;32m~/workdir/kaggle/santander/lib/model.py\u001b[0m in \u001b[0;36mtrain_lgbm_fold_classif\u001b[0;34m(df, df_test, features, df_target, df_stratify, keep_index, repeat_cv, n_splits, n_max_estimators, verbose_round, write)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 early_stopping_rounds=100)\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Code\n",
    "#keep_that = np.array(train[train['oof_preds_ref_error'] > 0.99].index)\n",
    "test['oof_preds_ref'] = 0\n",
    "remove_cols = ['target', 'ID_code', 'is_train', 'key_0', 'strat', 'oof_preds_ref', 'oof_preds_ref_error']\n",
    "features = list(set(train.columns) - set(remove_cols))\n",
    "\n",
    "        \n",
    "importances, df_oof_preds, df_preds, filename, models = train_lgbm_fold_classif(\n",
    "    train, test, features, train['target'],\n",
    "    train['target'],\n",
    "    #keep_index=keep_that,\n",
    "    repeat_cv=1, n_splits=4,\n",
    "    n_max_estimators=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importances(importances, num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = list(importances.groupby('feature')['gain'].mean().sort_values(ascending=False).head(10).index)\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['var_53', 'var_6', 'var_174', 'var_139', 'var_33', 'var_76', 'var_21', 'var_12', 'var_22', 'var_34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(train_test['var_68'])\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof_preds.index = train.index\n",
    "train['oof_preds'] = df_oof_preds\n",
    "train['oof_preds'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['abs_error'] = abs(train['target'] - train['oof_preds'])\n",
    "train['error'] = train['target'] - train['oof_preds']\n",
    "train['abs_error'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train[train['target'] == 1]['abs_error'])\n",
    "sns.distplot(train[train['target'] == 0]['abs_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['target'] == 1].sort_values(by='abs_error', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['target'] == 0].sort_values(by='abs_error', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_tabular\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    train[features].values,\n",
    "    feature_names=list(train.columns),\n",
    "    class_names=['0', '1'],\n",
    "    training_labels=train['target']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(models, data):\n",
    "    #feats = data.shape[0]\n",
    "    #data = np.reshape(data, (1, feats))\n",
    "    pred=np.zeros(data.shape[0])\n",
    "    n=len(models)\n",
    "    for i_model in models:\n",
    "        pred+=i_model.predict(data)/n\n",
    "    return pred\n",
    "\n",
    "def prob(data):\n",
    "    pred = model(models, data)\n",
    "    probas = np.array(list(zip(1-pred, pred)))\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 187082 # False prediction\n",
    "print('target:', train['target'].iloc[i])\n",
    "exp = explainer.explain_instance(train[features].iloc[i].values, prob)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 58770 # False prediction\n",
    "print('target:', train['target'].iloc[i])\n",
    "exp = explainer.explain_instance(train[features].iloc[i].values, prob)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 90698 # Good prediction\n",
    "print('target:', train['target'].iloc[i])\n",
    "exp = explainer.explain_instance(train[features].iloc[i].values, prob)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 79322 # Good prediction\n",
    "print('target:', train['target'].iloc[i])\n",
    "exp = explainer.explain_instance(train[features].iloc[i].values, prob)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['target']==0].sort_values(by='oof_preds_ref_error', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(models[1])\n",
    "shap_values = explainer.shap_values(train[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 187082 # False prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[i,:], train[features].iloc[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, train[features].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('var_6', shap_values, train[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('var_133', shap_values, train[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof_preds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof_preds.index = train.index\n",
    "train['oof_preds'] = df_oof_preds\n",
    "train['oof_preds'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "train['oof_preds'] = df_oof_preds\n",
    "train[['target', 'oof_preds']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['abs_error'] = abs(train['target'] - train['oof_preds'])\n",
    "train['error'] = train['target'] - train['oof_preds']\n",
    "train['abs_error'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train[train['target'] == 1]['abs_error'])\n",
    "sns.distplot(train[train['target'] == 0]['abs_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['target'] == 1].sort_values(by='abs_error', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['ID_code'] == idcode][features].values.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train['ID_code'] == idcode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idcode = 'train_187082' # False prediction\n",
    "print('target:', train[train['ID_code'] == idcode]['target'].values)\n",
    "exp = explainer.explain_instance(train[train['ID_code'] == idcode][features].values.ravel(), prob)\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['abs_error'] > 0.7]['target'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = 0.7\n",
    "print(\"base: 0.8981078597740635\")\n",
    "roc_auc_score(train[train['error'] < tr]['target'].values, (train[train['error'] < tr]['oof_preds'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['error'] > tr]['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['high_error'] = (train['error'] > tr).astype('int')\n",
    "train['high_error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "remove_cols = ['target', 'ID_code', 'high_error', 'error', 'oof_preds']\n",
    "features = list(set(train.columns) - set(remove_cols))\n",
    "\n",
    "importances, df_oof_preds, df_preds, filename = train_lgbm_fold_classif(\n",
    "                              train,\n",
    "                              train,\n",
    "    train['high_error'],\n",
    "                              features, train['high_error'],\n",
    "                              repeat_cv=1, n_splits=4,\n",
    "                              n_max_estimators=10000\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importances(importances, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lib.model import train_lgbm_fold_reg\n",
    "\n",
    "remove_cols = ['target', 'ID_code', 'high_error', 'error', 'oof_preds', 'abs_error']\n",
    "features = list(set(train.columns) - set(remove_cols))\n",
    "\n",
    "importances, df_oof_preds, df_preds = train_lgbm_fold_reg(\n",
    "                              train,\n",
    "                              test,\n",
    "                              features, train['error'],\n",
    "                              repeat_cv=1, n_splits=4,\n",
    "                              n_max_estimators=10000\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importances(importances, num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "tr=0.7\n",
    "print(train[train['error'] > tr].shape)\n",
    "\n",
    "subset = pd.DataFrame(train[train['error'] <= tr])\n",
    "\n",
    "remove_cols = ['target', 'ID_code', 'high_error', 'error', 'oof_preds']\n",
    "features = list(set(train.columns) - set(remove_cols))\n",
    "\n",
    "importances, df_oof_preds, df_preds, filename = train_lgbm_fold_classif(\n",
    "                              subset,\n",
    "                              test,\n",
    "                              features, subset['target'],\n",
    "                              repeat_cv=1, n_splits=4,\n",
    "                              n_max_estimators=10000\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT CV: 0 CV SCORE: 0.9751631824628406 TR SCORE 0.9993788178275096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importances(importances, num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = train.corr(method ='pearson') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs[['target', 'oof_preds', 'error']].sort_values(by=['error'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(x):\n",
    "    if x <= 0.5:\n",
    "        x = 0\n",
    "    if x > 0.5:\n",
    "        x = 1\n",
    "    return x\n",
    "\n",
    "preds_try = pd.DataFrame(df_oof_preds)\n",
    "preds_try['oof_error_pred'] = df_preds\n",
    "preds_try['preds_1'] = np.clip(preds_try[0], 0, 1)\n",
    "#preds_try['preds_1'] = np.round(preds_try[0])\n",
    "#preds_try['preds_1'] = preds_try[0].apply(lambda x: tune(x))\n",
    "print(\"base: 0.8981078597740635\")\n",
    "roc_auc_score(train['target'].values, (preds_try['preds_1'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_try = pd.DataFrame(df_oof_preds)\n",
    "preds_try.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ID_code'].reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_file = df_preds.merge(test['ID_code'].reset_index(), on=df_preds.index)[['ID_code', 0]].rename(columns={0:'target'})\n",
    "df_pred_file.to_csv('./data_tmp/test_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ID_code'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'preds_lgbm_classif_CV_0.88763_TR_0.98783'\n",
    "filename = 'preds_lgbm_classif_CV_0.89800_TR_0.91159'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_submission_from_hdf('preds_lgbm_classif_CV_0.89810_TR_0.91029', test['ID_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_lgbm_classif_CV_0.89810_TR_0.91029.hdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
